{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fU1jtZhK2xF5"
   },
   "outputs": [],
   "source": [
    "from HestonFFT import Call_Heston\n",
    "import numpy as np \n",
    "import scipy \n",
    "import math\n",
    "import scipy.integrate\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from NN import ModelLarge\n",
    "from TestPerformance import test_performanceNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eUwLlTpN28Gd"
   },
   "outputs": [],
   "source": [
    "#loading the training set\n",
    "df = pd.read_csv(\"HestonTrainXXL.csv\").to_numpy()[:,1:]\n",
    "y = df[:,0].reshape(-1,1)\n",
    "x = df[:,1:]\n",
    "train_x = torch.from_numpy(x).double()\n",
    "train_y = torch.from_numpy(y).reshape(-1,1).double() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BRMqsiJk2_4F"
   },
   "outputs": [],
   "source": [
    "#setting the hyperparameters of the model \n",
    "input_dim = 8\n",
    "hidden_dim = 128\n",
    "\n",
    "model = ModelLarge(input_dim, hidden_dim).to(device)\n",
    "model = model.double()\n",
    "\n",
    "#loss and learning rate \n",
    "criterion = nn.L1Loss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr = learning_rate) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                            optimizer, factor=0.5, \n",
    "                            patience=3, \n",
    "                            verbose=True ) \n",
    "\n",
    "\n",
    "#valutation set \n",
    "df = pd.read_csv(\"HestonTestNN.csv\").to_numpy()[:,1:]\n",
    "test_x = df[:,1:]\n",
    "test_y = df[:,0]\n",
    "test_y_Tensor = torch.from_numpy(test_y).to(device)\n",
    "test_x_Tensor = torch.from_numpy(test_x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nToy63OT3Eih",
    "outputId": "bac4c0c6-4da2-4195-a47b-fbb654a0542f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0    Loss: 12.627512205976977    Test Loss:  3.49e-03\n",
      "Epoch: 2    Loss: 3.676419880430981    Test Loss:  2.04e-03\n",
      "Epoch: 4    Loss: 3.1567868716615934    Test Loss:  2.20e-03\n",
      "Epoch: 6    Loss: 3.0400011673440424    Test Loss:  3.15e-03\n",
      "Epoch: 8    Loss: 2.616211071862402    Test Loss:  1.02e-03\n",
      "Epoch: 10    Loss: 2.537531762350157    Test Loss:  1.10e-03\n",
      "Epoch: 12    Loss: 2.122584266000305    Test Loss:  1.97e-03\n",
      "Epoch: 14    Loss: 2.080474675670011    Test Loss:  2.22e-03\n",
      "Epoch: 16    Loss: 1.9172571622331374    Test Loss:  1.26e-03\n",
      "Epoch: 18    Loss: 1.9120964486428884    Test Loss:  8.51e-04\n",
      "Epoch: 20    Loss: 2.008193127598748    Test Loss:  2.81e-03\n",
      "Epoch: 22    Loss: 1.6818483607980839    Test Loss:  1.68e-03\n",
      "Epoch: 24    Loss: 1.6782398814774422    Test Loss:  1.55e-03\n",
      "Epoch: 26    Loss: 1.9137522195086631    Test Loss:  1.76e-03\n",
      "Epoch: 28    Loss: 1.6591076086264376    Test Loss:  7.12e-04\n",
      "Epoch: 30    Loss: 1.641499524566649    Test Loss:  8.31e-04\n",
      "Epoch: 32    Loss: 1.454732862855155    Test Loss:  3.04e-03\n",
      "Epoch: 34    Loss: 1.5447701120267818    Test Loss:  6.27e-04\n",
      "Epoch: 36    Loss: 1.5536965996210623    Test Loss:  9.65e-04\n",
      "Epoch 00037: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch: 38    Loss: 0.7849039743050172    Test Loss:  5.20e-04\n",
      "Epoch: 40    Loss: 0.7250488620103317    Test Loss:  7.63e-04\n",
      "Epoch: 42    Loss: 0.6899705840904786    Test Loss:  4.79e-04\n",
      "Epoch: 44    Loss: 0.7365203871038988    Test Loss:  9.86e-04\n",
      "Epoch: 46    Loss: 0.6782447624310912    Test Loss:  5.04e-04\n",
      "Epoch: 48    Loss: 0.6885389027057166    Test Loss:  9.16e-04\n",
      "Epoch: 50    Loss: 0.7254868066753584    Test Loss:  6.40e-04\n",
      "Epoch 00051: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch: 52    Loss: 0.43487091039539305    Test Loss:  4.47e-04\n",
      "Epoch: 54    Loss: 0.4210520915221767    Test Loss:  6.52e-04\n",
      "Epoch: 56    Loss: 0.41154512017278033    Test Loss:  3.95e-04\n",
      "Epoch: 58    Loss: 0.3822020684667058    Test Loss:  3.50e-04\n",
      "Epoch: 60    Loss: 0.40313479333466445    Test Loss:  4.68e-04\n",
      "Epoch: 62    Loss: 0.3851839842941234    Test Loss:  4.34e-04\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch: 64    Loss: 0.31501364180459546    Test Loss:  3.12e-04\n",
      "Epoch: 66    Loss: 0.3091482507112949    Test Loss:  3.02e-04\n",
      "Epoch: 68    Loss: 0.30321892519253485    Test Loss:  3.28e-04\n",
      "Epoch: 70    Loss: 0.3006671315442145    Test Loss:  2.89e-04\n",
      "Epoch: 72    Loss: 0.29804416857169774    Test Loss:  3.06e-04\n",
      "Epoch: 74    Loss: 0.28850948758263856    Test Loss:  2.89e-04\n",
      "Epoch: 76    Loss: 0.29450955016329866    Test Loss:  2.90e-04\n",
      "Epoch: 78    Loss: 0.28603103155779225    Test Loss:  2.77e-04\n",
      "Epoch: 80    Loss: 0.2832432128097242    Test Loss:  3.37e-04\n",
      "Epoch: 82    Loss: 0.2819418870566081    Test Loss:  2.75e-04\n",
      "Epoch: 84    Loss: 0.2801454099581524    Test Loss:  3.40e-04\n",
      "Epoch: 86    Loss: 0.27616468223184976    Test Loss:  2.51e-04\n",
      "Epoch: 88    Loss: 0.27226175344230924    Test Loss:  2.84e-04\n",
      "Epoch: 90    Loss: 0.26878509816192614    Test Loss:  2.66e-04\n",
      "Epoch: 92    Loss: 0.27129993079855946    Test Loss:  2.70e-04\n",
      "Epoch: 94    Loss: 0.27221628012886046    Test Loss:  3.38e-04\n",
      "Epoch: 96    Loss: 0.26828507454838724    Test Loss:  2.58e-04\n",
      "Epoch: 98    Loss: 0.2722643342420187    Test Loss:  3.11e-04\n",
      "Epoch: 100    Loss: 0.25481661723614313    Test Loss:  3.09e-04\n",
      "Epoch: 102    Loss: 0.260332965423087    Test Loss:  2.96e-04\n",
      "Epoch: 104    Loss: 0.26050065146621426    Test Loss:  3.05e-04\n",
      "Epoch: 106    Loss: 0.25583802394432426    Test Loss:  3.22e-04\n",
      "Epoch: 108    Loss: 0.2496495503661729    Test Loss:  2.43e-04\n",
      "Epoch: 110    Loss: 0.24609690030045317    Test Loss:  2.82e-04\n",
      "Epoch: 112    Loss: 0.248619350033102    Test Loss:  2.47e-04\n",
      "Epoch: 114    Loss: 0.2457646145926989    Test Loss:  2.35e-04\n",
      "Epoch: 116    Loss: 0.24576812074803003    Test Loss:  2.63e-04\n",
      "Epoch: 118    Loss: 0.2380605227924557    Test Loss:  2.61e-04\n",
      "Epoch: 120    Loss: 0.2537212871656626    Test Loss:  2.29e-04\n",
      "Epoch: 122    Loss: 0.24427538057331452    Test Loss:  2.89e-04\n",
      "Epoch 00123: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch: 124    Loss: 0.21097901118940837    Test Loss:  2.13e-04\n",
      "Epoch: 126    Loss: 0.21087443852346313    Test Loss:  2.69e-04\n",
      "Epoch: 128    Loss: 0.20860176016527712    Test Loss:  2.39e-04\n",
      "Epoch: 130    Loss: 0.20374294986762734    Test Loss:  2.13e-04\n",
      "Epoch: 132    Loss: 0.20883349996963713    Test Loss:  2.08e-04\n",
      "Epoch: 134    Loss: 0.20378037295490653    Test Loss:  2.11e-04\n",
      "Epoch 00135: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch: 136    Loss: 0.19229067125127344    Test Loss:  2.11e-04\n",
      "Epoch: 138    Loss: 0.18902238928658507    Test Loss:  2.19e-04\n",
      "Epoch: 140    Loss: 0.18893329345968113    Test Loss:  2.07e-04\n",
      "Epoch: 142    Loss: 0.18945216181757618    Test Loss:  2.25e-04\n",
      "Epoch: 144    Loss: 0.18686390073220052    Test Loss:  2.11e-04\n",
      "Epoch: 146    Loss: 0.18666599936554987    Test Loss:  2.27e-04\n",
      "Epoch: 148    Loss: 0.18555781503842803    Test Loss:  2.15e-04\n",
      "Epoch: 150    Loss: 0.18464048267797928    Test Loss:  2.12e-04\n",
      "Epoch: 152    Loss: 0.18413668422268648    Test Loss:  2.10e-04\n",
      "Epoch: 154    Loss: 0.18464294021779512    Test Loss:  2.09e-04\n",
      "Epoch: 156    Loss: 0.1826016201497644    Test Loss:  2.14e-04\n",
      "Epoch: 158    Loss: 0.18341056617977552    Test Loss:  2.21e-04\n",
      "Epoch: 160    Loss: 0.18229968390217183    Test Loss:  1.99e-04\n",
      "Epoch: 162    Loss: 0.18195158752123597    Test Loss:  2.13e-04\n",
      "Epoch: 164    Loss: 0.1816809300637712    Test Loss:  2.10e-04\n",
      "Epoch: 166    Loss: 0.18008724213632674    Test Loss:  2.15e-04\n",
      "Epoch: 168    Loss: 0.17969039165741363    Test Loss:  2.11e-04\n",
      "Epoch: 170    Loss: 0.1787597845736661    Test Loss:  2.27e-04\n",
      "Epoch: 172    Loss: 0.17893936978963584    Test Loss:  2.10e-04\n",
      "Epoch: 174    Loss: 0.17777157877354505    Test Loss:  2.00e-04\n",
      "Epoch: 176    Loss: 0.17911952808120235    Test Loss:  2.05e-04\n",
      "Epoch: 178    Loss: 0.17800356862138014    Test Loss:  2.17e-04\n",
      "Epoch: 180    Loss: 0.17736174426134624    Test Loss:  2.12e-04\n",
      "Epoch: 182    Loss: 0.17690092156804188    Test Loss:  2.08e-04\n",
      "Epoch: 184    Loss: 0.17683450495028566    Test Loss:  2.06e-04\n",
      "Epoch: 186    Loss: 0.17497294309842268    Test Loss:  2.27e-04\n",
      "Epoch: 188    Loss: 0.17487819129609203    Test Loss:  2.05e-04\n",
      "Epoch: 190    Loss: 0.17401440283174466    Test Loss:  2.18e-04\n",
      "Epoch: 192    Loss: 0.17387878610966834    Test Loss:  2.02e-04\n",
      "Epoch: 194    Loss: 0.17353682312951907    Test Loss:  1.97e-04\n",
      "Epoch: 196    Loss: 0.17505381363303602    Test Loss:  2.03e-04\n",
      "Epoch: 198    Loss: 0.173340240119145    Test Loss:  1.95e-04\n",
      "Epoch: 200    Loss: 0.17277759458087116    Test Loss:  2.00e-04\n",
      "Epoch: 202    Loss: 0.17211917250439737    Test Loss:  2.05e-04\n",
      "Epoch: 204    Loss: 0.1716675339640041    Test Loss:  1.91e-04\n",
      "Epoch: 206    Loss: 0.17204773991738875    Test Loss:  2.12e-04\n",
      "Epoch: 208    Loss: 0.16975021314907196    Test Loss:  2.02e-04\n",
      "Epoch: 210    Loss: 0.17169064190133942    Test Loss:  1.95e-04\n",
      "Epoch: 212    Loss: 0.1697550696830287    Test Loss:  2.01e-04\n",
      "Epoch 00213: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch: 214    Loss: 0.16576590828710852    Test Loss:  1.84e-04\n",
      "Epoch: 216    Loss: 0.16513778330343057    Test Loss:  1.85e-04\n",
      "Epoch: 218    Loss: 0.16528622205339552    Test Loss:  1.82e-04\n",
      "Epoch: 220    Loss: 0.16416989751110556    Test Loss:  1.84e-04\n",
      "Epoch: 222    Loss: 0.16478586038858808    Test Loss:  1.84e-04\n",
      "Epoch: 224    Loss: 0.1634686423783483    Test Loss:  1.82e-04\n",
      "Epoch: 226    Loss: 0.1631512219628588    Test Loss:  1.87e-04\n",
      "Epoch: 228    Loss: 0.16364960673582266    Test Loss:  1.84e-04\n",
      "Epoch: 230    Loss: 0.16332200542373018    Test Loss:  1.84e-04\n",
      "Epoch: 232    Loss: 0.1622867926833868    Test Loss:  1.86e-04\n",
      "Epoch: 234    Loss: 0.1624041577391749    Test Loss:  1.86e-04\n",
      "Epoch: 236    Loss: 0.16198030849495762    Test Loss:  1.83e-04\n",
      "Epoch: 238    Loss: 0.16168890197073493    Test Loss:  1.80e-04\n",
      "Epoch: 240    Loss: 0.16154552845823764    Test Loss:  1.84e-04\n",
      "Epoch: 242    Loss: 0.16128229433396232    Test Loss:  1.83e-04\n",
      "Epoch: 244    Loss: 0.16093217876430088    Test Loss:  1.80e-04\n",
      "Epoch: 246    Loss: 0.16083825856860837    Test Loss:  1.82e-04\n",
      "Epoch: 248    Loss: 0.1604408889514207    Test Loss:  1.83e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250    Loss: 0.16074647169147238    Test Loss:  1.80e-04\n",
      "Epoch: 252    Loss: 0.15975813894869087    Test Loss:  1.80e-04\n",
      "Epoch: 254    Loss: 0.16000039760240375    Test Loss:  1.79e-04\n",
      "Epoch: 256    Loss: 0.15976167322124785    Test Loss:  1.85e-04\n",
      "Epoch 00257: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch: 258    Loss: 0.15566603537516768    Test Loss:  1.82e-04\n",
      "Epoch: 260    Loss: 0.15536932603135234    Test Loss:  1.82e-04\n",
      "Epoch: 262    Loss: 0.155248863175337    Test Loss:  1.82e-04\n",
      "Epoch: 264    Loss: 0.1549065387238027    Test Loss:  1.83e-04\n",
      "Epoch: 266    Loss: 0.15489060544208152    Test Loss:  1.82e-04\n",
      "Epoch: 268    Loss: 0.15459567923530165    Test Loss:  1.83e-04\n",
      "Epoch: 270    Loss: 0.15450894531803236    Test Loss:  1.82e-04\n",
      "Epoch: 272    Loss: 0.15445950368832273    Test Loss:  1.83e-04\n",
      "Epoch: 274    Loss: 0.15430608755632888    Test Loss:  1.81e-04\n",
      "Epoch: 276    Loss: 0.1542745667143972    Test Loss:  1.80e-04\n",
      "Epoch: 278    Loss: 0.15421343762899814    Test Loss:  1.83e-04\n",
      "Epoch: 280    Loss: 0.15371500148502237    Test Loss:  1.81e-04\n",
      "Epoch: 282    Loss: 0.15356603830269558    Test Loss:  1.80e-04\n",
      "Epoch: 284    Loss: 0.15354639028433956    Test Loss:  1.80e-04\n",
      "Epoch: 286    Loss: 0.15345736554536502    Test Loss:  1.81e-04\n",
      "Epoch: 288    Loss: 0.15342150525423034    Test Loss:  1.79e-04\n",
      "Epoch: 290    Loss: 0.15318627733310675    Test Loss:  1.80e-04\n",
      "Epoch: 292    Loss: 0.15332655448471103    Test Loss:  1.80e-04\n",
      "Epoch: 294    Loss: 0.15303991029239897    Test Loss:  1.78e-04\n",
      "Epoch: 296    Loss: 0.15296262090327198    Test Loss:  1.80e-04\n",
      "Epoch: 298    Loss: 0.15276979417924583    Test Loss:  1.76e-04\n",
      "Wall time: 14min 31s\n"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "if train: \n",
    "    %%time \n",
    "    num_epochs = 300\n",
    "    iter = 0\n",
    "    N = train_x.shape[0]\n",
    "    batch = 100\n",
    "\n",
    "    train_x = train_x.to(device)\n",
    "    train_y = train_y.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        index = torch.randperm(N)\n",
    "        train_input_shuffled = train_x[index]\n",
    "        train_target_shuffled = train_y[index]\n",
    "        s = 0\n",
    "\n",
    "        for b in range(0, N, batch):\n",
    "            x = train_x.narrow(0, b, batch)\n",
    "            y = train_y.narrow(0, b, batch)\n",
    "\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            outputs = model(x)\n",
    "            # print(y, outputs)\n",
    "            # Calculate Loss:\n",
    "            loss = criterion(outputs, y)\n",
    "            # print(loss)\n",
    "            s += loss\n",
    "\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            iter += 1\n",
    "        if epoch % 2 == 0:\n",
    "            test = f\"Test Loss:  {test_performanceNN(test_x_Tensor , test_y_Tensor , model).item():.2e}\"\n",
    "            print(f\"Epoch: {epoch}    Loss: {s.mean()}    {test}\")\n",
    "        # scheduler wll reduce the learning rate if gor 5 epoch there is no gain in term of the loss\n",
    "        scheduler.step(s.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jVubvpWvvl74"
   },
   "outputs": [],
   "source": [
    "#saving the model\n",
    "#torch.save(model, \"modelNNLarge.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HestonTestS.csv\n",
      "Time:  0.012998819351196289\n",
      "AAE:  6.09e-05\n",
      "MAE:  1.46e-03\n",
      "\n",
      "\n",
      "HestonTestM.csv\n",
      "Time:  0.02700042724609375\n",
      "AAE:  5.69e-05\n",
      "MAE:  8.60e-04\n",
      "\n",
      "\n",
      "HestonTestL.csv\n",
      "Time:  0.05307912826538086\n",
      "AAE:  5.28e-05\n",
      "MAE:  9.55e-04\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loading the model trained on 1milion observations  and look at the performance +\n",
    "mod = torch.load(\"modelNNLarge.pt\")\n",
    "\n",
    "datasets = [\"HestonTestS.csv\", \"HestonTestM.csv\", \"HestonTestL.csv\"]\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    df = pd.read_csv(dataset).to_numpy()[:,1:]\n",
    "    X = df[:,1:]\n",
    "    y = df[:,0]\n",
    "    test_y_Tensor = torch.from_numpy(y).to(device)\n",
    "    test_x_Tensor = torch.from_numpy(X).to(device)\n",
    "    test_performanceNN(test_x_Tensor, test_y_Tensor, mod, type_ = \"both\", to_return = False, time_ = True)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HestonTestS.csv\n",
      "Time:  0.019085168838500977\n",
      "AAE:  1.78e-04\n",
      "MAE:  2.34e-03\n",
      "\n",
      "\n",
      "HestonTestM.csv\n",
      "Time:  0.026999711990356445\n",
      "AAE:  1.76e-04\n",
      "MAE:  1.34e-03\n",
      "\n",
      "\n",
      "HestonTestL.csv\n",
      "Time:  0.054001808166503906\n",
      "AAE:  1.71e-04\n",
      "MAE:  2.67e-03\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loading the model trained on 100k observations  and look at the performance +\n",
    "mod = torch.load(\"modelNNLarge_SmallTrain.pt\")\n",
    "\n",
    "datasets = [\"HestonTestS.csv\", \"HestonTestM.csv\", \"HestonTestL.csv\"]\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    df = pd.read_csv(dataset).to_numpy()[:,1:]\n",
    "    X = df[:,1:]\n",
    "    y = df[:,0]\n",
    "    test_y_Tensor = torch.from_numpy(y).to(device)\n",
    "    test_x_Tensor = torch.from_numpy(X).to(device)\n",
    "    test_performanceNN(test_x_Tensor, test_y_Tensor, mod, type_ = \"both\", to_return = False, time_ = True)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
